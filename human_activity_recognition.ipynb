{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlights from Data Description:\n",
    "* 561 features\n",
    "* Data is normalized and features are bound within [-1,1]\n",
    "\n",
    "Therefore it is not necessary to scale the data; however, due to the number of features a method of dimensionality reduction must be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data as pandas dataframe, correctly assign feature names and target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alros\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.py:678: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n",
      "C:\\Users\\alros\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.py:678: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    }
   ],
   "source": [
    "X_names = pd.read_csv('UCI HAR Dataset/features.txt', sep='\\s+', names=['id','feature'])\n",
    "X_names = X_names['feature'].tolist()\n",
    "\n",
    "X_train = pd.read_csv('UCI HAR Dataset/train/X_train.txt', sep='\\s+',names=X_names)\n",
    "X_test = pd.read_csv('UCI HAR Dataset/test/X_test.txt', sep='\\s+', names=X_names)\n",
    "\n",
    "\n",
    "label_map = {row.id: row.label for i, row in pd.read_csv('UCI HAR Dataset/activity_labels.txt', sep = '\\s+', \n",
    "                                                         names=['id','label']).iterrows()}\n",
    "\n",
    "y_train = pd.read_csv('UCI HAR Dataset/train/y_train.txt', sep='\\s+', names=['activity_id'])\n",
    "y_test = pd.read_csv('UCI HAR Dataset/test/y_test.txt', sep='\\s+', names=['activity_id'])\n",
    "\n",
    "y_train['activity']=y_train.activity_id.map(label_map)\n",
    "y_test['activity']=y_test.activity_id.map(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Duplicates\n",
    "* There are duplicate labels, but this is not an issue since we are not going to be using the label names\n",
    "* There are duplicate columns. This is likely due to the data being normalized. These columns will be dealt with automatically through dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of curiosity, let's take a look at the duplicate column labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tBodyAccMag-mean()', 'tBodyAccMag-sma()'),\n",
       " ('tBodyAccMag-std()', 'tGravityAccMag-std()'),\n",
       " ('tBodyAccMag-mad()', 'tGravityAccMag-mad()'),\n",
       " ('tBodyAccMag-max()', 'tGravityAccMag-max()'),\n",
       " ('tBodyAccMag-min()', 'tGravityAccMag-min()'),\n",
       " ('tBodyAccMag-sma()', 'tGravityAccMag-mean()'),\n",
       " ('tBodyAccMag-energy()', 'tGravityAccMag-energy()'),\n",
       " ('tBodyAccMag-iqr()', 'tGravityAccMag-iqr()'),\n",
       " ('tBodyAccMag-entropy()', 'tGravityAccMag-entropy()'),\n",
       " ('tBodyAccMag-arCoeff()1', 'tGravityAccMag-arCoeff()1'),\n",
       " ('tBodyAccMag-arCoeff()2', 'tGravityAccMag-arCoeff()2'),\n",
       " ('tBodyAccMag-arCoeff()3', 'tGravityAccMag-arCoeff()3'),\n",
       " ('tBodyAccMag-arCoeff()4', 'tGravityAccMag-arCoeff()4'),\n",
       " ('tGravityAccMag-mean()', 'tGravityAccMag-sma()'),\n",
       " ('tBodyAccJerkMag-mean()', 'tBodyAccJerkMag-sma()'),\n",
       " ('tBodyGyroMag-mean()', 'tBodyGyroMag-sma()'),\n",
       " ('tBodyGyroJerkMag-mean()', 'tBodyGyroJerkMag-sma()'),\n",
       " ('fBodyAccMag-mean()', 'fBodyAccMag-sma()'),\n",
       " ('fBodyBodyAccJerkMag-mean()', 'fBodyBodyAccJerkMag-sma()'),\n",
       " ('fBodyBodyGyroMag-mean()', 'fBodyBodyGyroMag-sma()'),\n",
       " ('fBodyBodyGyroJerkMag-mean()', 'fBodyBodyGyroJerkMag-sma()')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = []\n",
    "dup = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    for j in range(X_train.shape[1]):\n",
    "        if np.array_equal(X_train.iloc[:,i].values, X_train.iloc[:,j].values) and i!=j and i not in index and j not in index:\n",
    "            index.append(i)\n",
    "            dup.append((X_train.keys()[i],X_train.keys()[j]))\n",
    "dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the label names, it does seem likely that the duplicate columns are produced due to normalization\n",
    "As mentioned earlier, the duplicate columns do not need to be removed since applying dimensionality reduction methods will inherently take care of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_));\n",
    "plt.title('Explained Variance vs. Number of PCs')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the plot it is clear that:\n",
    "* ~70 components account for 95% of the variance\n",
    "* 100 components account for ~98% of the variance\n",
    "* 200 components account for nearly 100% of the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize first 10 Principal Components and Make Performance Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names =  list('PC {}'.format(i) for i in range(1,11))\n",
    "PCA_10 = pd.DataFrame(X_train_pca[:,:10], columns=names)\n",
    "PCA_10['activity'] = y_train.activity\n",
    "\n",
    "sns.pairplot(PCA_10, hue='activity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the plot it seems that:\n",
    "* Laying will be the most easily classified action\n",
    "* It may be difficult for classifiers to differentiate between standing and sitting\n",
    "* It may be difficult for classifier to differentiate between the 3 walking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier performance will be tested using:\n",
    "* 10 components - to confirm or reject hypothesis made based on the above visualization \n",
    "* 50 components\n",
    "* 100 compoments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca10 = PCA(n_components = 10)\n",
    "pca50 = PCA(n_components = 50)\n",
    "pca100 = PCA(n_components = 100)\n",
    "\n",
    "pca10.fit(X_train)\n",
    "pca50.fit(X_train)\n",
    "pca100.fit(X_train)\n",
    "\n",
    "X_train_10 = pca10.transform(X_train)\n",
    "X_test_10 = pca10.transform(X_test)\n",
    "X_train_50 = pca50.transform(X_train)\n",
    "X_test_50 = pca50.transform(X_test)\n",
    "X_train_100 = pca100.transform(X_train)\n",
    "X_test_100 = pca100.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers:\n",
    "* LDA - [inspiration](https://www.researchgate.net/publication/309373991_Modeling_Human_Activity_Recognition_by_Dimensionality_Reduction_Approach) for this method\n",
    "* Logistic Regression\n",
    "* MLPClassifier\n",
    "* KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import classifiers\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lr = LogisticRegression(random_state = 0)\n",
    "mlp = MLPClassifier(random_state = 0)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "def train_predict(classifiers, X_train, X_test, y_train, y_test):\n",
    "    from sklearn import metrics\n",
    "    cm = {}\n",
    "    acc = {}\n",
    "    labels = list(np.unique(y_train))\n",
    "    for clf in classifiers:\n",
    "        name = clf.__class__.__name__\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        cm[name] = metrics.confusion_matrix(y_test, y_pred, labels=labels)\n",
    "        acc[name] = metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return cm, acc\n",
    "\n",
    "\n",
    "classifiers = [lda, lr, mlp, knn]\n",
    "name_order = [clf.__class__.__name__ for clf in classifiers]\n",
    "\n",
    "cm_10, acc_10 = train_predict(classifiers, X_train_10, X_test_10, y_train.activity, y_test.activity)\n",
    "cm_50, acc_50 = train_predict(classifiers, X_train_50, X_test_50, y_train.activity, y_test.activity)\n",
    "cm_100, acc_100 = train_predict(classifiers, X_train_100, X_test_100, y_train.activity, y_test.activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_order = [name.__class__.__name__ for name in classifiers]\n",
    "labels = list(np.unique(y_train.activity))\n",
    "\n",
    "f, axes = plt.subplots(4, 3, sharex=True, sharey=True, figsize=(13,13))\n",
    "\n",
    "for name, i in zip(name_order, range(4)):\n",
    "    sns.heatmap(cm_10[name], cmap='BuGn', annot=True, fmt='g', cbar=False, square=True, xticklabels=labels, \n",
    "            yticklabels=labels,ax=axes[i,0]);\n",
    "    axes[i,0].set_title('{} - 10 PCs'.format(name));\n",
    "    \n",
    "for name, i in zip(name_order, range(4)):\n",
    "    sns.heatmap(cm_50[name], cmap='BuGn', annot=True, fmt='g', cbar=False, square=True, xticklabels=labels, \n",
    "            yticklabels=labels,ax=axes[i,1]);\n",
    "    axes[i,1].set_title('{} - 50 PCs'.format(name));\n",
    "    \n",
    "for name, i in zip(name_order, range(4)):\n",
    "    sns.heatmap(cm_100[name], cmap='BuGn', annot=True, fmt='g', cbar=False, square=True, xticklabels=labels, \n",
    "            yticklabels=labels,ax=axes[i,2]);\n",
    "    axes[i,2].set_title('{} - 100 PCs'.format(name));    \n",
    "    \n",
    "plt.setp(axes[3,0].get_xticklabels(), rotation=30);\n",
    "plt.setp(axes[3,1].get_xticklabels(), rotation=30);\n",
    "plt.setp(axes[3,2].get_xticklabels(), rotation=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis from PCA visualization confirmed\n",
    "From the above confusion matrices it is clear that the classifiers had the most difficulty distinguishing between the activites labeled sitting and standing as well as all of the activities involving walking.\n",
    "\n",
    "Of particular interest is the observation that all of the classifiers aside from KNN misclassified walking upstairs as walking more frequently than they misclassified walking downstairs as walking when using 50 and 100 PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for acc, pcs in zip([acc_10, acc_50, acc_100], [10,50,100]):\n",
    "    print '\\nAccuracy using {} PCs:'.format(pcs)\n",
    "    for name in name_order:\n",
    "        print '{}: {:.4f}'.format(name, acc[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
